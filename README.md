SwiftUI Core ML + ARKit Object Detection
About
A sample iOS application that combines ARKit’s augmented reality features with real-time image classification using Apple’s MobileNetV2 Core ML model. Built with SwiftUI.

Features
ARKit-powered camera view.

Live classification of AR camera frames using Core ML.

Augmented overlays for detected objects.

SwiftUI-based user interface.

Setup
Clone the project.

Download MobileNetV2.mlmodel from Apple and add to Xcode.

Set NSCameraUsageDescription in Info.plist.

Usage
Run on an ARKit-capable device (iPhone/iPad).

Point camera at real-world objects; detected labels appear in AR view.

Resources
Apple Core ML Models

ARKit Developer Docs

Vision Framework Docs

Example tutorials: [Better Programming blog walkthrough], [SwiftUI + Core ML basic repo]

License
MIT
